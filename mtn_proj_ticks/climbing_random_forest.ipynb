{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from category_encoders import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append('/kaggle/mtn_proj_ticks/')\n",
    "import climbing_ticks_helper as helper\n",
    "importlib.reload(helper)\n",
    "pd.set_option('display.expand_frame_repr', False) # display full data in terminal\n",
    "%matplotlib inline\n",
    "bar = 'bar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for response variable Attempts:\n",
      "          precision  recall  f1-score  support\n",
      "1              0.87    0.97      0.91   176.00\n",
      "2              0.10    0.04      0.06    26.00\n",
      "3              0.00    0.00      0.00     1.00\n",
      "4              0.00    0.00      0.00     3.00\n",
      "accuracy       0.83    0.83      0.83     0.83\n",
      "\n",
      "\n",
      "Classification report for response variable Lead Style:\n",
      "          precision  recall  f1-score  support\n",
      "0              0.85    0.95      0.90   149.00\n",
      "1              0.40    0.32      0.36    25.00\n",
      "2              0.37    0.22      0.27    32.00\n",
      "accuracy       0.76    0.76      0.76     0.76\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('/kaggle/mtn_proj_ticks/model_ready_ticks.csv')\n",
    "\n",
    "# first take at a Random Forest model:\n",
    "X = df.drop(columns=['Attempts', 'Lead Style'])\n",
    "y = df[['Attempts', 'Lead Style']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=y_test.columns)\n",
    "\n",
    "# compute accuracy, recall, precision, f1 score for multi class predictions\n",
    "# Generate classification reports for each response variable\n",
    "for resp in y_test.columns:\n",
    "    print(f\"Classification report for response variable {resp}:\")\n",
    "    # don't print macro avg or weighted avg\n",
    "    cr = classification_report(y_test[resp], y_pred_df[resp], zero_division=0, output_dict=True)\n",
    "    cr.pop('macro avg', None)\n",
    "    cr.pop('weighted avg', None)\n",
    "    prettytable = pd.DataFrame(cr).T\n",
    "    print(prettytable.round(2))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_readable = pd.read_csv('/kaggle/mtn_proj_ticks/grouped_ticks.csv')\n",
    "df_readable = df_readable[df_readable['RouteID'].isin(X_test['RouteID'])].set_index('RouteID').loc[X_test['RouteID']].reset_index()\n",
    "df_combined = helper.combine_predictions_with_data(df_readable, y_pred_df)\n",
    "\n",
    "# print all rows that got Lead Style wrong\n",
    "wrong_preds = df_combined[df_combined['Lead Style'] != df_combined['Predicted Lead Style']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# * categorize predictions that are close to the correct answer. EG: predicting an Onsight/Flash when the actual is a Redpoint with only 2 attempts isn't that bad. but predicting an Onsight/Flash when the actual is a Fell/Hung with 5 attempts is bad.\n",
    "# * categorize predictions that though I woudl do better, vs predictions that thought I would do worse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
