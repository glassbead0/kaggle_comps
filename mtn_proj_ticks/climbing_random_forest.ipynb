{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from category_encoders import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append('/kaggle/mtn_proj_ticks/')\n",
    "import climbing_ticks_helper as helper\n",
    "importlib.reload(helper)\n",
    "pd.set_option('display.expand_frame_repr', False) # display full data in terminal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for response variable Attempts:\n",
      "              precision  recall  f1-score  support\n",
      "1                  0.90    1.00      0.95   229.00\n",
      "2                  0.00    0.00      0.00    22.00\n",
      "3                  0.00    0.00      0.00     3.00\n",
      "4                  0.00    0.00      0.00     2.00\n",
      "5                  0.00    0.00      0.00     1.00\n",
      "accuracy           0.89    0.89      0.89     0.89\n",
      "macro avg          0.18    0.20      0.19   257.00\n",
      "weighted avg       0.80    0.89      0.84   257.00\n",
      "\n",
      "\n",
      "Classification report for response variable Lead Style:\n",
      "              precision  recall  f1-score  support\n",
      "0                  0.77    0.92      0.84   163.00\n",
      "1                  0.00    0.00      0.00    28.00\n",
      "2                  1.00    0.11      0.20     9.00\n",
      "3                  0.62    0.61      0.62    57.00\n",
      "accuracy           0.72    0.72      0.72     0.72\n",
      "macro avg          0.60    0.41      0.41   257.00\n",
      "weighted avg       0.66    0.72      0.68   257.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('/kaggle/mtn_proj_ticks/ticks_prepped.csv')\n",
    "\n",
    "# first take at a Random Forest model:\n",
    "X = df.drop(columns=['Attempts', 'Lead Style'])\n",
    "y = df[['Attempts', 'Lead Style']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=y_test.columns)\n",
    "\n",
    "# compute accuracy, recall, precision, f1 score for multi class predictions\n",
    "# Generate classification reports for each response variable\n",
    "for resp in y_test.columns:\n",
    "    print(f\"Classification report for response variable {resp}:\")\n",
    "    prettytable = pd.DataFrame(classification_report(y_test[resp], y_pred_df[resp], zero_division=0, output_dict=True)).T\n",
    "    print(prettytable.round(2))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# reorder columns\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoute\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRouteID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoute Type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlpine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSafety\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg Stars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPitches\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Lead Style\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLead Style\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Attempts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempts\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 21\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_predictions_with_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mcombine_predictions_with_data\u001b[0;34m(original_df, response_cols, predictions)\u001b[0m\n\u001b[1;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLead Style\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLead Style\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43moe\u001b[49m\u001b[38;5;241m.\u001b[39minverse_transform(df)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# add Route and RouteID back to the df\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmerge(df_grouped[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoute\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRouteID\u001b[39m\u001b[38;5;124m'\u001b[39m]], left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'oe' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_combined = combine_predictions_with_data(df, y_test.columns, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
